---
title: "Lab 6a Quiz"
author: "Will Scheib"
date: "10/22/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(glmnet)
```

## 1
1. multicollinearity is present (at least one predictor is a linear combination of other predictors), and/or
2. when the ratio of observations to number of predictors is small

## 2
Makes it worse

## 3
Ridge, because some coefficients might be artificially forced to zero in lasso.

## 4
```{r}
##model.matrix automatically transform categorical variables into dummy codes, which is needed as the glmnet function cannot handle categorical variables
##remove first column
x<-model.matrix(Apps~.,data=College)[,-1]

##store the response variable
y<-College$Apps

##split data
set.seed(2019)
sample.data<-sample.int(nrow(College), floor(.50*nrow(College)), replace = F) ##observations to belong to the training data
x.train<-x[sample.data,]
x.test<-x[-sample.data,]
y.train<-y[sample.data]
y.test<-y[-sample.data]
```

## 5
glmnet() can't handle factors.

## 6
```{r}
##use CV to find optimal lambda based on training set
set.seed(4630)
cv.out<-glmnet::cv.glmnet(x.train,y.train,alpha=0, thresh = 1e-23)
bestlam<-cv.out$lambda.min
bestlam
```

## 7
```{r}
plot(cv.out)
```

## 8
```{r}
##fit ridge regression using training data and bestlam
ridge.mod<-glmnet::glmnet(x.train,y.train,alpha=0,lambda=bestlam, thresh = 1e-25)

##Test MSE with best lambda
ridge.pred<-predict(ridge.mod,newx=x.test)
mean((ridge.pred-y.test)^2)
```

## 9
```{r}
set.seed(4630)
cv.out.lasso<-glmnet::cv.glmnet(x.train,y.train,alpha=1, thresh = 1e-23)
bestlam.lasso<-cv.out.lasso$lambda.min
bestlam.lasso
```

```{r}
plot(cv.out.lasso)
```

```{r}
##fit ridge regression using training data
lasso.mod<-glmnet::glmnet(x.train,y.train,alpha=1,lambda=bestlam.lasso, thresh = 1e-23)

##Test MSE with best lambda
lasso.pred<-predict(lasso.mod,newx=x.test)
mean((lasso.pred-y.test)^2)
```

## 10
```{r}
result<-lm(Apps~.,data=data.frame(x.train, Apps=y.train))
result.pred<-predict(result,newdata = data.frame(x.test))
mean((result.pred-y.test)^2)
```

## 11
Ridge < Lasso < OLS. Not surprising.

## 12
```{r}
## Ridge
grid<-10^seq(10,-2,length=100)
out.all<-glmnet::glmnet(x,y,alpha=0,lambda=grid,thresh = 1e-23)
plot(out.all, xvar = "lambda")
abline(v=log(bestlam), lty=2)
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)

## Lasso
grid<-10^seq(10,-2,length=100)
out.all<-glmnet::glmnet(x,y,alpha=1,lambda=grid,thresh = 1e-23)
plot(out.all, xvar = "lambda")
abline(v=log(bestlam.lasso), lty=2)
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)
```
As the value of log lambda increases, the coefficients grow closer to zero.
